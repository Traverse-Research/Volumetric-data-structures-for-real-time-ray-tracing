\section{Requirements} \label{requirements}
\subsection{Asset size} \label{requirements:asset_size}
\subsection{Sampling speed} \label{requirements:sampling_speed}
\subsection{Animation playback} \label{requirements:animation_playback}
\subsection{Lossy compression} \label{requirements:lossy_compression}
\subsection{Level of detail} \label{requirements:level_of_detail}
%This section is split up into two parts, namely functional and non functional requirements. The former describes certain functionalities which the program should have, and the latter describes how it does said things and how well it does said things. Below, the functional and non functional requirements are listed along with a description and reasoning of each requirement.
%
%\subsection{Functional Requirements (FR)}\label{requirements:FR}
%\begin{enumerate}
%    \item \textbf{The data structure needs to be traversable by \href{https://traverseresearch.nl/}{Traverse}'s rendering framework on the GPU.} To make the project useful for \href{https://traverseresearch.nl/}{Traverse} (and to make implementation easier), the project needs to be written in the existing Breda framework. This will require adhering to everything which was discussed in Section \ref{introduction:traverse}. However, when benchmarking against the current state of the art (NanoVDB\cite{museth2021nanovdb}) it will be too time consuming to implement entirely in \href{https://traverseresearch.nl/}{Traverse}'s framework. So NanoVDB will be evaluated in Cuda/c++. \label{FR:framework}
%    \item \textbf{Every cell in the data structure must be able to hold a certain set of values.} For our volumetric data structure to be useful it somehow needs to store data about its volume. This can be the density of particles, the material that is being traversed or the brightness of light emission. For example, if we traverse a cloud we only really care about the density, but when traversing an explosion we also care about the light emitted by the fire. So there needs to be a way to have the leaves of our data structure contain arbitrary data (with a known size at compile time), while not impacting the ray marching performance. Note that it is enough for every voxel to have the same number of properties, there is no need for a varying number of properties within one volume. Making sure the performance stays roughly the same with an increase in the number of properties, can be done by using bitfields for intersection testing and only once an intersection has been found, we use a pointer to go to the actual data of our leaf node. This way the efficiency of caching wont degrade as our leafs get more attributes. \label{FR:datalayout}
%    \item \textbf{The data needs to retain the resolution it was imported from.} It is important that we optimize the ray tracing performance while retaining the original resolution. Not only might there be issues regarding the expected visuals and the actual visuals if we start altering the amount of detail. But we can also run into graphical artifacts.
%          As described in \ref{introduction:voxel_data_structures}, we might be able to speed up the rendering process by getting the max and min density values in a certain area as close to each other as possible. So it might be worth investigating if this can be done, and if the benefits in performance outweigh the reduction in quality. \label{FR:resolution}
%    \item \textbf{Interface with OpenVDB\cite{museth2013vdb}, the industry standard for volumetric data exchange.} This is the go-to method for generating/storing/distributing volumetric data and thus very important if we want to interface with tools like Houdini\cite{Houdini}. We need to be able to read the VDB files from disk, then store that data in a VDB data structure in RAM, and then transform that into our final data structure. The VDB portion of this pipeline already exists. We just need to create a transform to our desired data structure. \label{FR:interfacing}
%    \item \textbf{Animation needs to be possible without copying volume data back and forth from the GPU.} In general, syncing data from the GPU to the CPU can be a major cause of idling as the CPU waits for the data to be transferred. Another issue has to do with syncing the data. Lets say there is a simulation running on the GPU, but the CPU retrieved the volume data to modify it in some way. Now once the CPU uploads its data back to the GPU, it will overwrite anything the GPU just did. So not only is this copying of data a performance but also a programming problem. In this case animation includes at least a simple fluid or particle system, along with the built in animation for OpenVDB. \label{FR:animation}
%\end{enumerate}
%
%
%
%
%
%\subsection{Non-functional requirements (NFR)}\label{requirements:NFR}
%\begin{enumerate}
%    \item \textbf{The most important part of this research, is to test the viability of animation.} For clouds, it might be possible to use static volumes. But for almost anything related to physics, we need the structure to be dynamic. Explosions, fire or dust storms require animation and thus the volume data needs to be efficiently editable. This means that not only do we need the values within each voxel to be changed, but the entire structure of our volume needs to be able to change. When using a sparse data structure, new nodes must be allocated during the simulation as we need them. As a guideline we are aiming to reliably update 100k voxels every frame, while not sacrificing the sparsity of the used data structure.\label{NFR:update}
%    \item \textbf{Secondary is the ray tracing performance.} Traverse Research is a real time graphics company. So everything they do has to, at some point, fit into a few milliseconds per frame. Given that volumes can have a resolution of $16^3$ to $1024^3+$ voxels and there can be millions of rays entering the volume every frame, we need a way to speed up the process of traversing. Some research has already been done in this area, as described in Section\ref{introduction:volume_acceleration}. This is not the most important part of the project as the optimization process of ray tracing techniques can become very open ended. Along with that, adding animation to the existing GPU data structures would push volumetric data structure research forward in a more significant way. \label{NFR:tracing}
%    \item \textbf{Third is the memory footprint.} Memory footprint is often tied to ray tracing performance due to caches, but not necessarily. In modern engines, the GPU needs to keep many different resources in memory. These are resources like acceleration structures, textures and buffers. So to keep the solution viable in a large game engine, we need a small data structure. \label{NFR:memory}
%    \item \textbf{Lastly, the data needs to be sparse to reduce the memory footprint of homogeneous areas in given volumes.} Although this is closely tied to NFR \ref{NFR:tracing} and NFR\ref{NFR:memory}, we wanted to emphasize the necessity of sparsity. This sparsity will allow the performance to scale up and down depending on scene complexity. It would be wasteful if a tiny cloud in a large volume would require the same amount of memory as a cloud which would fill the entire volume. \label{NFR:sparsity}
%\end{enumerate}
%
%\subsection{Concretization}\label{requirements:concretization}
%These requirements lead us to the main goal of this research, a sparse dynamic hierarchical data structure for volumes. Now we do run into an issue with the definition of animation, as we want to support both simulation and playback of VDB animations. When we focus on simulation, we need a shallow tree which can quickly allocate all required nodes. Preferably, able to simulate millions of particles and insert all of these quickly into our data structure. When we want a data structure for animation, we have to focus on data size. We can, for example, store each animation frame into separate trees. But  we quickly run into memory issues this way, so we need to somehow re-use information from our tree across frames. This will be done using a similar algorithm for node de-duplication as described in the SVDAG paper \cite{kampe2013high}. Fortunately we can take as long as we want for this pre-processing step. Now both of these solutions will result in sparse trees which will be traversable on the GPU, and have the same architecture as VDB. We will use a dynamic top node, then at least one internal layer of nodes, and then the leaves which contain the highest resolution data. Furthermore, we will make use of bitmasks to quickly check the existence of child nodes, and support arbitrary data in the internal nodes to support LOD's and local min/max data.