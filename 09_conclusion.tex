\section{Conclusion} \label{conclusion}
Taking a look back at our research questions from section \ref{research_questions}, we ended up with decent answers to both. In the question we already mentioned that there is not a single structure that can do it all. There is always a mix of trade-offs. So one thing we can do is optimize the parts that are the same for the desired structures and then build specialized structures around that. This method is described in section \ref{approach:flipbook_animations}. During this thesis, a VDB structure was implemented as it was a balance between structure size, sample timing and efficient HDDA. This structure along with our voxel data compression scheme allowed us to load in full animation sequences into our engine and render them at movie-level fidelity. The block compression scheme allows for a $8\times$ reduction in memory usage over the standard 16-bit floating point format. Along with that, we can get another reduction by between $14\%$ and $57\%$ without significant quality loss by clustering voxel bricks which contain roughly the same data. Both of these compression methods do not work for all models out of the box, some models might need to be slightly altered to prevent edge cases as seen in \ref{fig:block_compression:fire_unorm}. In table \ref{tab:tracing_performance:numbers} we see that our main bottleneck when rendering volumes using a sampling technique is computation (ALU bound), instead of memory. The same problem arises with our HDDA implementation, which has low occupancy due to high register usage, but this is not an issue as long as we are not memory-bound. Both of these results are surprising as GPU algorithms are most often memory-bound due to the large number of pixels and buffers that have to be read and written to. So when optimizing the implemented methods the main point of interest would be to reduce the number of steps taken and reduce the register usage of our HDDA implementation. The former can likely be achieved by utilizing a signed distance field as described in section \ref{related_work:attribute_separation:fast_volume_traversal}.

\subsection{Future research} \label{conclusion:future_research}

\begin{itemize}
    \item One point of improvement to the in section \ref{approach:clustering_similar_nodes} mentioned method could be a more optimized rejection scheme. Currently, we reject if the normalized variance of all brick values is too high. This only allows us to cluster homogeneous bricks and not bricks that contain a smooth gradient, even though there are bricks that are simply a gradient in one specific axis which likely could be clustered. Another major issue with this compression scheme is the build times. It currently takes multiple minutes to compress both the shockwave and the chimney animation, which makes development iteration times very slow.
    \item Different texture compression schemes can be experimented with. Certain platforms, like mobile, have different texture compression formats namely, Ericsson Texture Compression (ETC) and Adaptive scalable compression (ASTC). ETC has 1 bit per voxel which is twice the compression ratio of the method described in \ref{approach:texture_compression}. And ASTC can compress between $0,15$ and $1,19$ depending on certain options. These compression schemes will change the quality of our data but might be worth it on the supported platforms.
    \item Different tree topologies can be experimented with. Most notably B+Tree's with uniform layer sizes. This might make it possible to deduplicate many of the bit masks and thus shrink the tree size. Another benefit as mentioned in \cite{hoetzlein2016gvdb}, by using a tree where all internal nodes are $8^3$ we get better ray tracing performance.
    \item The ideas about running simulations inside our VDB data structure as described in section \ref{approach:simulation} can be implemented. During this research, there was a very limited time investment in making this feature work. So experimenting with the theorized technique could result in a nice piece of follow-up research.
\end{itemize}

\subsection{Renders}

\begin{figure}[H]
    \centering
    \subfloat[Path traced half resolution Disney cloud path traced using multiple scattering.]{
        \includegraphics[width=0.9\textwidth]{figures/disney _cloud.png} \label{fig:renders:disney_cloud}
    }
    \hfill
    \subfloat[Ray marched smoke inside the Bistro scene coming from a chimney.]{
        \includegraphics[width=0.45\textwidth]{figures/bistro_chimney_0.png} \label{fig:renders:bistro_0}
    }
    \hfill
    \subfloat[The same scene later on the day shows a different frame in the animation sequence.]{
        \includegraphics[width=0.45\textwidth]{figures/bistro_chimney_1.png} \label{fig:renders:bistro_1}
    }

    \hfill

    \caption{Multiple renders made in Breda using the created data structures. Figure \ref{fig:renders:disney_cloud} shows the largest tested model which was rendered by using path tracing. This was not real-time (about 5 frames per second) because we use multiple scattering, which means that our ray can bounce many times into different directions within the cloud. Figures \ref{fig:renders:bistro_0} and \ref{fig:renders:bistro_1} show two frames from a video made in the Bistro scene. This was rendered using our ray marcher which does run in real-time together with all other systems in Breda. The way that the light dynamically scatters throughout the chimney smoke is an effect that is currently not done in any AAA game.} \label{fig:renders}
\end{figure}